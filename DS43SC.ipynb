{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS43SC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielleRomanoff/DS-Unit-4-Sprint-3-Neural-Networks/blob/master/DS43SC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Y6SKlgYrpcym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Networks Sprint Challenge"
      ]
    },
    {
      "metadata": {
        "id": "BrEbRrjVphPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1) Define the following terms:\n",
        "\n",
        "- Neuron - *A basic unit of computation in a neural network. Called a node or unit. It receives input from some other nodes or external sources.*\n",
        "- Input Layer - *The layer receives input for the dataset. Might be referred to as the visible layer bcause it's the only part that interacts with the data and is exposed to it.*\n",
        "- Hidden Layer - *The layers that come after the input layer but before the output layer. Hidden layers cannot be accessed except through the input layer and are not interacted with*\n",
        "- Output Layer - *This is the final layer. The output layer provides a vector of values. Usually it's modified by an activation function to put it into a format that works for our context.*\n",
        "- Activation - *The activation function introduces non-linearity into the output. Most of our lives are not linear. Therefore, an activation function takes an output value and performs a mathematical operation on it.*\n",
        "- Backpropagation - *Improves the accuracy of predictions in data mining and machine learning. Neural Networks use it as an algorithm to find gradient descent with respect to the weights. Here, the weights are updated backwards from output to input."
      ]
    },
    {
      "metadata": {
        "id": "Q5EksLqnp4oB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " YOUR ANSWER HERE"
      ]
    },
    {
      "metadata": {
        "id": "Ri_gRA2Jp728",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2) Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
        "\n",
        "| x1 | x2 | x3 | y |\n",
        "|----|----|----|---|\n",
        "| 1  | 1  | 1  | 1 |\n",
        "| 1  | 0  | 1  | 0 |\n",
        "| 0  | 1  | 1  | 0 |\n",
        "| 0  | 0  | 1  | 0 |"
      ]
    },
    {
      "metadata": {
        "id": "Ig6ZTH8tpQ19",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# linear algebra\n",
        "import numpy as np \n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "# data processing\n",
        "import pandas as pd \n",
        "\n",
        "# data visualization\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.optimizers import SGD, Adam, Nadam\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split,cross_val_score, StratifiedKFold, KFold, GridSearchCV \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsNJbEWHE1q2",
        "colab_type": "code",
        "outputId": "b948aee4-e07a-4c6e-86dc-8645f71cdcba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "inputs = np.array([[1, 1, 1, 1],\n",
        "    [1, 1., 0, 1],\n",
        "    [1, 0., 1, 1],\n",
        "    [1, 0, 0, 1]])\n",
        "\n",
        "# Ideal outputs\n",
        "correct_outputs = [[1],\n",
        "    [0],\n",
        "    [0],\n",
        "   [0]]\n",
        "\n",
        "print('Inputs are: \\n', inputs)\n",
        "print('Correct Output is: ',correct_outputs)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs are: \n",
            " [[1. 1. 1. 1.]\n",
            " [1. 1. 0. 1.]\n",
            " [1. 0. 1. 1.]\n",
            " [1. 0. 0. 1.]]\n",
            "Correct Output is:  [[1], [0], [0], [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NjUtP6glE1zL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wumiIcmUE11j",
        "colab_type": "code",
        "outputId": "c70c3699-1bc3-4769-a961-6ec7b35fb7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Random weights\n",
        "weights = 2 * np.random.random((4,1))-1\n",
        "weights"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.50773023],\n",
              "       [-0.40693517],\n",
              "       [-0.36498742],\n",
              "       [-0.69987018]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "metadata": {
        "id": "EPJMHJhDE15X",
        "colab_type": "code",
        "outputId": "4a7a8599-0882-4b4c-c754-d110bf220c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        " # Weighted sum of inputs and weights\n",
        "weighted_sum = np.dot(inputs, weights)\n",
        "weighted_sum"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.97952299],\n",
              "       [-1.61453557],\n",
              "       [-1.57258782],\n",
              "       [-1.2076004 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "metadata": {
        "id": "RK-lbEvjE18P",
        "colab_type": "code",
        "outputId": "96d2d602-825c-478a-c65a-48a7c30d96d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Activate with sigmoid function\n",
        "activated_output = sigmoid(weighted_sum)\n",
        "activated_output"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.1213697 ],\n",
              "       [0.16595986],\n",
              "       [0.17184779],\n",
              "       [0.23012591]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "metadata": {
        "id": "0Vy7d4LGGG9Z",
        "colab_type": "code",
        "outputId": "9a0508f2-8f44-4d74-9f83-08e7d5d70a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "  # Calculate Error\n",
        "error = correct_outputs - activated_output\n",
        "error"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.8786303 ],\n",
              "       [-0.16595986],\n",
              "       [-0.17184779],\n",
              "       [-0.23012591]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "metadata": {
        "id": "9qs7BVemGG_v",
        "colab_type": "code",
        "outputId": "1f25b469-dedb-45c6-d957-a2894cc1c669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Calculate weight adjustments with sigmoid_derivative\n",
        "adjustments = error * sigmoid_derivative(activated_output)  # Gradient Descent\n",
        "adjustments"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.21885064],\n",
              "       [-0.04120559],\n",
              "       [-0.04264632],\n",
              "       [-0.05677646]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "metadata": {
        "id": "spX_Z21GGVJz",
        "colab_type": "code",
        "outputId": "d8f5c668-6652-43b1-ac8a-976249302f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        " # Update weights\n",
        "weights += np.dot(inputs.T, adjustments)\n",
        "weights"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.42950796],\n",
              "       [-0.22929012],\n",
              "       [-0.1887831 ],\n",
              "       [-0.62164791]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "metadata": {
        "id": "HzZv1eLqGVMY",
        "colab_type": "code",
        "outputId": "aa319d96-b709-460e-fc23-dc2ef3bee76f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "for iteration in range(500):\n",
        "  \n",
        "  # Weighted sum of inputs and weights\n",
        "  weighted_sum = np.dot(inputs, weights)\n",
        "  \n",
        "  # Activate with sigmoid function\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  \n",
        "  # Calculate Error\n",
        "  error = correct_outputs - activated_output\n",
        "  \n",
        "  # Calculate weight adjustments with sigmoid_derivative\n",
        "  adjustments = error * sigmoid_derivative(activated_output)\n",
        "  \n",
        "  # Update weights\n",
        "  weights += np.dot(inputs.T, adjustments)\n",
        "  \n",
        "print('Optimized weights after training: ')\n",
        "print(weights)\n",
        "\n",
        "print(\"Output After Training:\")\n",
        "print(activated_output)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimized weights after training: \n",
            "[[-4.89277504]\n",
            " [ 6.41984394]\n",
            " [ 6.4198519 ]\n",
            " [-5.084915  ]]\n",
            "Output After Training:\n",
            "[[9.45834016e-01]\n",
            " [2.77638387e-02]\n",
            " [2.77640551e-02]\n",
            " [4.66993027e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "86HyRi8Osr3U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3) Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. \n",
        "- Your network must have one hidden layer. \n",
        "- You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
        "- Train your model on the Heart Disease dataset from UCI:\n",
        "\n",
        "[Github Dataset](https://github.com/ryanleeallred/datasets/blob/master/heart.csv)\n",
        "\n",
        "[Raw File on Github](https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv)\n"
      ]
    },
    {
      "metadata": {
        "id": "CNfiajv3v4Ed",
        "colab_type": "code",
        "outputId": "9d663e1b-b014-4716-9973-f7762298230c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "heart = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "heart.head()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "metadata": {
        "id": "iqXidEBqIFjL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "34319035-c321-4cc0-e9ec-9955e76202da"
      },
      "cell_type": "code",
      "source": [
        "# Checking for nulls Yay NONE\n",
        "heart.isnull().sum()"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "cp          0\n",
              "trestbps    0\n",
              "chol        0\n",
              "fbs         0\n",
              "restecg     0\n",
              "thalach     0\n",
              "exang       0\n",
              "oldpeak     0\n",
              "slope       0\n",
              "ca          0\n",
              "thal        0\n",
              "target      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "metadata": {
        "id": "zfUS1J8oIFtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "8bb2dcd2-c0f2-4f34-9992-3a36e3fd0231"
      },
      "cell_type": "code",
      "source": [
        "pos_target =  'target'\n",
        "neg_target =  'negative'\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
        "women = heart[heart['sex'] == 0]\n",
        "men =heart[heart['sex'] == 1]\n",
        "\n",
        "ax = sns.distplot(women[women['target'] == 1], bins=18, color=sns.diverging_palette(128, 240, n=14) , label = pos_target, ax = axes[0], kde =False)\n",
        "ax = sns.distplot(women[women['target'] == 0], bins=40, color=sns.diverging_palette(128, 240, n=14), label = neg_target, ax = axes[0], kde =False)\n",
        "ax.legend()\n",
        "ax.set_title('Female')\n",
        "current_palette = sns.color_palette()\n",
        "ax = sns.distplot(men[men['target'] == 1], bins=18, color=current_palette_14, label = pos_target, ax = axes[0], kde =False)\n",
        "ax = sns.distplot(men[men['target'] == 0], bins=40, color=sns.diverging_palette(128, 240, n=14) , label = neg_target, ax = axes[1], kde = False )\n",
        "ax.legend()\n",
        "_ = ax.set_title('Male')"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py:6521: MatplotlibDeprecationWarning: \n",
            "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
            "  alternative=\"'density'\", removal=\"3.1\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAEHCAYAAABY5snMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "E62bQk3SIFvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "60b1292c-3c5b-428b-df01-cdcf644cc490"
      },
      "cell_type": "code",
      "source": [
        "sns.barplot(x='cp', y='target', data=heart)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1428: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
            "  stat_data = remove_na(group_data)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fce96688cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Ayy-UJlkIFx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1390c4d6-0806-4729-ad2b-0c6325448e86"
      },
      "cell_type": "code",
      "source": [
        "y = heart['target']\n",
        "X= heart.drop(['target'], axis=1)\n",
        "# Splitting data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# Confirming correct shapes of data\n",
        "print(\"X_train shape is: \", (X_train.shape))\n",
        "print(\"X_test shape is: \",(X_test.shape))\n",
        "print(\"y_train shape is: \",(y_train.shape))\n",
        "print(\"y_trest shape is: \",(y_test.shape))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape is:  (242, 13)\n",
            "X_test shape is:  (61, 13)\n",
            "y_train shape is:  (242,)\n",
            "y_trest shape is:  (61,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DXWWLlMJWp3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "  def __init__(self):\n",
        "    self.inputs =13\n",
        "    self.hiddenNodes = 4\n",
        "    self.outputNodes = 1\n",
        "    \n",
        "    # Initlize Weights\n",
        "    self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) # (3x4)\n",
        "    self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes) # (4x1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s0GMB-hJWqAB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "f84e1c4d-465e-4029-9be9-d4858423f912"
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network()\n",
        "\n",
        "print(\"Layer 1 weights: \\n\", NN.L1_weights)\n",
        "print(\"Layer 2 weights: \\n\", NN.L2_weights)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 1 weights: \n",
            " [[-3.54521916e-01 -1.62847641e+00 -5.61145572e-01 -9.08664273e-01]\n",
            " [-5.54690867e-01  1.06835571e+00 -1.23371630e+00  3.26544197e-01]\n",
            " [ 1.21132047e+00  4.53728606e-01 -2.96290464e-02  1.94976743e+00]\n",
            " [ 1.35318292e-01  1.24805561e-01  6.38011922e-01 -1.15768779e+00]\n",
            " [ 4.20497179e-01 -6.83406715e-01 -1.93778857e+00  5.98887621e-01]\n",
            " [ 6.54738031e-01  2.77977673e-01  7.14947340e-01 -8.58146452e-02]\n",
            " [ 1.33982670e+00  2.20780192e+00  2.46823271e-01  1.19710956e+00]\n",
            " [-8.56841516e-01  1.32035098e+00  5.94422212e-01  2.21180703e+00]\n",
            " [-1.03471193e+00 -6.80625364e-01 -5.48852925e-01  9.20270660e-01]\n",
            " [ 7.64720720e-01 -2.14655405e+00  2.90888243e-01 -5.87342691e-01]\n",
            " [ 4.04541356e-02 -2.84164481e+00 -3.47945221e-01  1.08159347e-02]\n",
            " [ 7.80430449e-01  2.12999049e-03  9.79055776e-02 -1.14835968e+00]\n",
            " [ 4.53734982e-01 -1.51818676e+00 -9.77118635e-01 -3.86725446e-01]]\n",
            "Layer 2 weights: \n",
            " [[-0.74914759]\n",
            " [-0.74686157]\n",
            " [ 1.0583499 ]\n",
            " [-1.30340621]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B5gDvmEzIF0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Neural_Network(object):\n",
        "    def __init__(self):\n",
        "        self.inputs = 13\n",
        "        self.hiddenNodes = 4\n",
        "        self.outputNodes =1\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.L1_weights = np.random.randn(self.inputs, self.hiddenNodes) #(13x4)\n",
        "#       self.L2_weights = np.random.randn(self.hiddenNodes, self.hiddenNodes)\n",
        "        self.L2_weights = np.random.randn(self.hiddenNodes, self.outputNodes)  #(4x1)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        # Weighted sum between inputs and hidden layer\n",
        "        self.hidden_sum = np.dot(X, self.L1_weights)\n",
        "        # Activations on weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        # Weighted sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.L2_weights)\n",
        "        # final activation of output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        return self.activated_output\n",
        "    \n",
        "    def sigmoid(self, s):\n",
        "        return 1/(1+np.exp(-s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jm6VqWWiIF3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NN = Neural_Network()\n",
        "output = NN.feed_forward(x)\n",
        "print(\"output: \", output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85aYwrfvUS9N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Backpropogation \n",
        "print(\"Inputs: \\n\", X)\n",
        "print(\"Initial Weights \\n\", NN.L1_weights)\n",
        "print(\"Weighted Sum 1 \\n\", NN.hidden_sum)\n",
        "print(\"Activations from first layer: \\n\", NN.activated_hidden)\n",
        "print(\"Second layer Weights \\n\", NN.L2_weights)\n",
        "print(\"Weighted Sum 2 \\n\", NN.output_sum)\n",
        "print(\"Predictions \\n\", output)\n",
        "print(\"Correct Output: \\n\", y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GGT1oRzXw3H9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4) Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy. \n",
        "\n",
        "- Use the Heart Disease Dataset (binary classification)\n",
        "- Use an appropriate loss function for a binary classification task\n",
        "- Use an appropriate activation function on the final layer of your network. \n",
        "- Train your model using verbose output for ease of grading.\n",
        "- Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
        "- When hyperparameter tuning, show you work by adding code cells for each new experiment. \n",
        "- Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
        "- You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
      ]
    },
    {
      "metadata": {
        "id": "TI90XOXkg-H6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "f62ecdba-1408-48cc-b372-7bfa2b8caf2e"
      },
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "heart = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
        "heart.head()"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   0     1       1  \n",
              "1   0     2       1  \n",
              "2   0     2       1  \n",
              "3   0     2       1  \n",
              "4   0     2       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "metadata": {
        "id": "XWw4IYxLxKwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1171245c-595c-4131-e357-d232f40502b6"
      },
      "cell_type": "code",
      "source": [
        "# Splitting the data set\n",
        "y = heart['target']\n",
        "X= heart.drop(['target'], axis=1)\n",
        "# Splitting data into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Confirming correct shapes of data\n",
        "print(\"X_train shape is: \", (X_train.shape))\n",
        "print(\"X_test shape is: \",(X_test.shape))\n",
        "print(\"y_train shape is: \",(y_train.shape))\n",
        "print(\"y_trest shape is: \",(y_test.shape))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape is:  (242, 13)\n",
            "X_test shape is:  (61, 13)\n",
            "y_train shape is:  (242,)\n",
            "y_trest shape is:  (61,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z2GQstaYhDKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dfc60a8c-0cf0-476f-db29-49b9740465e8"
      },
      "cell_type": "code",
      "source": [
        "y_train = np.reshape(y_train, (-1,1))\n",
        "y_test = np.reshape(y_test, (-1,1))"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
            "  return getattr(obj, method)(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QBNDHfAAgYz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Normalize the data (all features should have roughly the same scale)\n",
        "normalized_X_train = preprocessing.normalize(X_train)\n",
        "normalized_X_test = preprocessing.normalize(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UP29uLWFjX3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# global hyperparameters\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mSqNv-XKavkY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(13, activation='relu', input_shape=(13,)))\n",
        "model.add(Dense(13, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kLEhgPmJavmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "547727da-42ea-462f-bc7d-7288969c1b59"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse', optimizer='adam', metrics=['mean_squared_error'])\n",
        "model.summary()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_251 (Dense)            (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_252 (Dense)            (None, 13)                182       \n",
            "_________________________________________________________________\n",
            "dense_253 (Dense)            (None, 1)                 14        \n",
            "=================================================================\n",
            "Total params: 378\n",
            "Trainable params: 378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hLXkZdrKavow",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dozru6P1avrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "301243ef-1174-4582-c5a3-a0a5a5c8802c"
      },
      "cell_type": "code",
      "source": [
        "RMSE_train = np.sqrt(history.history['mean_squared_error'])\n",
        "RMSE_test = np.sqrt(history.history['val_mean_squared_error'])\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(RMSE_train, label = 'Train')\n",
        "ax.plot(RMSE_test, label = 'Test')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('RMSE, Thousands of dollars')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEM+f9U5ZLtKVhKiwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "oPWeW5eZavvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2325f03c-6c59-4c7a-b787-fb9ac93b8e20"
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print('RMSE from NN: ', RMSE)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE from NN:  1.2148293660998961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ATXvu-Rav0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "y_pred = linreg.predict(X_test)\n",
        "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wmcT4o-Eavx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11c11e63-2779-49aa-833f-1963d78b0503"
      },
      "cell_type": "code",
      "source": [
        "print('RMSE from Linear Regression: ', RMSE)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE from Linear Regression:  0.34098492624865423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wWB1qD_3avtq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model \n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "def create_model(lr=0.05,\n",
        "                 activation='relu',                 \n",
        "                 input_shape=(X_train.shape[1],),\n",
        "                 optimizer=SGD,\n",
        "                 relu_alpha = 0.003,\n",
        "                 dropout_rate = 0.2,\n",
        "                weight_initializer='random_normal'):\n",
        "    \n",
        "    # initialize a model\n",
        "    model = Sequential()\n",
        "    \n",
        "    # add input layer\n",
        "    model.add(Dense(10, input_shape=input_shape, kernel_initializer=weight_initializer,))\n",
        "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    \n",
        "    # add hidden layers\n",
        "    model.add(Dense(10, kernel_initializer=weight_initializer,))\n",
        "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "        \n",
        "    model.add(Dense(10, kernel_initializer=weight_initializer,))\n",
        "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    \n",
        "    model.add(Dense(8, kernel_initializer=weight_initializer,))\n",
        "    model.add(LeakyReLU(alpha=relu_alpha)) \n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    \n",
        "    # add final output layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # optimizer\n",
        "    optimizer=optimizer(lr=lr)\n",
        "    \n",
        "    # compile model\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
        "              \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fNhET-xkYDG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1123
        },
        "outputId": "52d6b644-225a-44ed-859d-6b86e8868a35"
      },
      "cell_type": "code",
      "source": [
        "# baseline model\n",
        "epochs = 10\n",
        "batch_size = 100\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, \n",
        "                               epochs=epochs,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=1)\n",
        "kfold = StratifiedKFold(n_splits=3, random_state=42)\n",
        "results = cross_val_score(model, X, y, cv=kfold)\n",
        "print(f\"K-fold Cross-Val Results - Mean: {results.mean()} StDev: {results.std()} MSE\")"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "202/202 [==============================] - 5s 23ms/step - loss: 0.6933 - acc: 0.5198\n",
            "Epoch 2/10\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.6941 - acc: 0.4554\n",
            "Epoch 3/10\n",
            "202/202 [==============================] - 0s 91us/step - loss: 0.6937 - acc: 0.4653\n",
            "Epoch 4/10\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.6921 - acc: 0.5446\n",
            "Epoch 5/10\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.6907 - acc: 0.5446\n",
            "Epoch 6/10\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.6898 - acc: 0.5446\n",
            "Epoch 7/10\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.6907 - acc: 0.5446\n",
            "Epoch 8/10\n",
            "202/202 [==============================] - 0s 48us/step - loss: 0.6910 - acc: 0.5446\n",
            "Epoch 9/10\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6928 - acc: 0.5396\n",
            "Epoch 10/10\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.6902 - acc: 0.5446\n",
            "101/101 [==============================] - 2s 18ms/step\n",
            "Epoch 1/10\n",
            "202/202 [==============================] - 4s 22ms/step - loss: 0.6924 - acc: 0.5347\n",
            "Epoch 2/10\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6905 - acc: 0.5446\n",
            "Epoch 3/10\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.6911 - acc: 0.5446\n",
            "Epoch 4/10\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6913 - acc: 0.5446\n",
            "Epoch 5/10\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.6930 - acc: 0.5149\n",
            "Epoch 6/10\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6929 - acc: 0.5396\n",
            "Epoch 7/10\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6930 - acc: 0.5396\n",
            "Epoch 8/10\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6910 - acc: 0.5446\n",
            "Epoch 9/10\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.6887 - acc: 0.5446\n",
            "Epoch 10/10\n",
            "202/202 [==============================] - 0s 52us/step - loss: 0.6903 - acc: 0.5446\n",
            "101/101 [==============================] - 2s 19ms/step\n",
            "Epoch 1/10\n",
            "202/202 [==============================] - 5s 23ms/step - loss: 0.6935 - acc: 0.4505\n",
            "Epoch 2/10\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6944 - acc: 0.4406\n",
            "Epoch 3/10\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6960 - acc: 0.4554\n",
            "Epoch 4/10\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6929 - acc: 0.4802\n",
            "Epoch 5/10\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.6931 - acc: 0.4653\n",
            "Epoch 6/10\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6933 - acc: 0.4851\n",
            "Epoch 7/10\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6930 - acc: 0.5396\n",
            "Epoch 8/10\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.6918 - acc: 0.5446\n",
            "Epoch 9/10\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6914 - acc: 0.5594\n",
            "Epoch 10/10\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.6913 - acc: 0.5495\n",
            "101/101 [==============================] - 2s 19ms/step\n",
            "K-fold Cross-Val Results - Mean: 0.5445544672484445 StDev: 0.0 MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BG0yl_qJkYMx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13942
        },
        "outputId": "84d22b81-1857-4b45-f06b-57373f31a89e"
      },
      "cell_type": "code",
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
        "              'epochs': [20]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 15ms/step - loss: 0.6548 - acc: 0.6832\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 211us/step - loss: 0.6222 - acc: 0.6832\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 191us/step - loss: 0.6512 - acc: 0.6832\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 196us/step - loss: 0.6467 - acc: 0.6832\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 208us/step - loss: 0.6317 - acc: 0.6832\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 190us/step - loss: 0.6599 - acc: 0.6832\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 202us/step - loss: 0.6318 - acc: 0.6832\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 191us/step - loss: 0.6289 - acc: 0.6832\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 192us/step - loss: 0.6384 - acc: 0.6832\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 211us/step - loss: 0.6312 - acc: 0.6832\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 197us/step - loss: 0.6218 - acc: 0.6832\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 243us/step - loss: 0.6140 - acc: 0.6832\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 189us/step - loss: 0.6286 - acc: 0.6832\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 204us/step - loss: 0.6221 - acc: 0.6832\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 241us/step - loss: 0.6246 - acc: 0.6832\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 212us/step - loss: 0.6327 - acc: 0.6832\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 191us/step - loss: 0.6295 - acc: 0.6832\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 196us/step - loss: 0.6207 - acc: 0.6832\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 188us/step - loss: 0.6292 - acc: 0.6832\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 192us/step - loss: 0.6312 - acc: 0.6832\n",
            "101/101 [==============================] - 1s 11ms/step\n",
            "202/202 [==============================] - 0s 120us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 14ms/step - loss: 0.6924 - acc: 0.5396\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 190us/step - loss: 0.6950 - acc: 0.4752\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 191us/step - loss: 0.6942 - acc: 0.4752\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 181us/step - loss: 0.6939 - acc: 0.5050\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 191us/step - loss: 0.6948 - acc: 0.5050\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 192us/step - loss: 0.6914 - acc: 0.5891\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 221us/step - loss: 0.6897 - acc: 0.5743\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 187us/step - loss: 0.6907 - acc: 0.5149\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 198us/step - loss: 0.6853 - acc: 0.5594\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 187us/step - loss: 0.6960 - acc: 0.5248\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 186us/step - loss: 0.6898 - acc: 0.5347\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 205us/step - loss: 0.6877 - acc: 0.5743\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 186us/step - loss: 0.6866 - acc: 0.5693\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 195us/step - loss: 0.6845 - acc: 0.5891\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 206us/step - loss: 0.6781 - acc: 0.5792\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 208us/step - loss: 0.6842 - acc: 0.6386\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 187us/step - loss: 0.6955 - acc: 0.5297\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 206us/step - loss: 0.6740 - acc: 0.5842\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 182us/step - loss: 0.6725 - acc: 0.6238\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 207us/step - loss: 0.6807 - acc: 0.5842\n",
            "101/101 [==============================] - 1s 11ms/step\n",
            "202/202 [==============================] - 0s 126us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 14ms/step - loss: 0.6439 - acc: 0.7970\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 184us/step - loss: 0.5701 - acc: 0.8168\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 194us/step - loss: 0.5103 - acc: 0.8168\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 189us/step - loss: 0.4950 - acc: 0.8168\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 186us/step - loss: 0.4918 - acc: 0.8168\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 190us/step - loss: 0.4974 - acc: 0.8168\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 196us/step - loss: 0.4911 - acc: 0.8168\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 220us/step - loss: 0.4992 - acc: 0.8168\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 194us/step - loss: 0.4859 - acc: 0.8168\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 189us/step - loss: 0.4928 - acc: 0.8168\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 199us/step - loss: 0.4846 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 189us/step - loss: 0.4841 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 212us/step - loss: 0.4787 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 194us/step - loss: 0.4760 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 230us/step - loss: 0.4800 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 181us/step - loss: 0.4760 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 198us/step - loss: 0.4701 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 180us/step - loss: 0.4790 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 190us/step - loss: 0.4808 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 200us/step - loss: 0.4817 - acc: 0.8168\n",
            "101/101 [==============================] - 1s 12ms/step\n",
            "202/202 [==============================] - 0s 120us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 15ms/step - loss: 0.6721 - acc: 0.6832\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 119us/step - loss: 0.6424 - acc: 0.6832\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 129us/step - loss: 0.6400 - acc: 0.6832\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 130us/step - loss: 0.6345 - acc: 0.6832\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 132us/step - loss: 0.6330 - acc: 0.6832\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 125us/step - loss: 0.6262 - acc: 0.6832\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 125us/step - loss: 0.6262 - acc: 0.6832\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 125us/step - loss: 0.6378 - acc: 0.6832\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 127us/step - loss: 0.6278 - acc: 0.6832\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 137us/step - loss: 0.6280 - acc: 0.6832\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 124us/step - loss: 0.6196 - acc: 0.6832\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 117us/step - loss: 0.6257 - acc: 0.6832\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 126us/step - loss: 0.6308 - acc: 0.6832\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 111us/step - loss: 0.6242 - acc: 0.6832\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 114us/step - loss: 0.6253 - acc: 0.6832\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 126us/step - loss: 0.6305 - acc: 0.6832\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 125us/step - loss: 0.6287 - acc: 0.6832\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 130us/step - loss: 0.6290 - acc: 0.6832\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 123us/step - loss: 0.6267 - acc: 0.6832\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 123us/step - loss: 0.6315 - acc: 0.6832\n",
            "101/101 [==============================] - 1s 12ms/step\n",
            "202/202 [==============================] - 0s 81us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 15ms/step - loss: 0.6944 - acc: 0.5000\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 146us/step - loss: 0.6943 - acc: 0.4851\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 125us/step - loss: 0.6923 - acc: 0.5000\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 121us/step - loss: 0.6932 - acc: 0.5099\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 129us/step - loss: 0.6932 - acc: 0.5050\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 123us/step - loss: 0.6915 - acc: 0.5248\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 124us/step - loss: 0.6897 - acc: 0.5347\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 140us/step - loss: 0.6910 - acc: 0.5347\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 166us/step - loss: 0.6936 - acc: 0.4752\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 152us/step - loss: 0.6936 - acc: 0.4950\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 122us/step - loss: 0.6904 - acc: 0.5594\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 122us/step - loss: 0.6858 - acc: 0.5743\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 127us/step - loss: 0.6930 - acc: 0.5248\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 137us/step - loss: 0.6965 - acc: 0.4950\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 118us/step - loss: 0.6951 - acc: 0.5000\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 124us/step - loss: 0.6937 - acc: 0.4851\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 123us/step - loss: 0.6929 - acc: 0.5099\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 127us/step - loss: 0.6939 - acc: 0.5248\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 112us/step - loss: 0.6946 - acc: 0.4752\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 117us/step - loss: 0.6935 - acc: 0.4950\n",
            "101/101 [==============================] - 1s 12ms/step\n",
            "202/202 [==============================] - 0s 77us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 16ms/step - loss: 0.6278 - acc: 0.7772\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 154us/step - loss: 0.5646 - acc: 0.8168\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 153us/step - loss: 0.5479 - acc: 0.8168\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 163us/step - loss: 0.5220 - acc: 0.8168\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 153us/step - loss: 0.5044 - acc: 0.8168\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 155us/step - loss: 0.5089 - acc: 0.8168\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 167us/step - loss: 0.4937 - acc: 0.8168\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 136us/step - loss: 0.5149 - acc: 0.8168\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 135us/step - loss: 0.5212 - acc: 0.8168\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 137us/step - loss: 0.4784 - acc: 0.8168\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 158us/step - loss: 0.5204 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 173us/step - loss: 0.5026 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 172us/step - loss: 0.5200 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 174us/step - loss: 0.4996 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 165us/step - loss: 0.4852 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 145us/step - loss: 0.4971 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 158us/step - loss: 0.4979 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 176us/step - loss: 0.4946 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 162us/step - loss: 0.4843 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 167us/step - loss: 0.4938 - acc: 0.8168\n",
            "101/101 [==============================] - 1s 14ms/step\n",
            "202/202 [==============================] - 0s 105us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 16ms/step - loss: 0.6769 - acc: 0.6832\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 90us/step - loss: 0.6442 - acc: 0.6832\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 85us/step - loss: 0.6654 - acc: 0.6832\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 89us/step - loss: 0.6422 - acc: 0.6832\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.6392 - acc: 0.6832\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 84us/step - loss: 0.6200 - acc: 0.6832\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 96us/step - loss: 0.6569 - acc: 0.6832\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 97us/step - loss: 0.6373 - acc: 0.6832\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.6172 - acc: 0.6832\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.6363 - acc: 0.6832\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 88us/step - loss: 0.6375 - acc: 0.6832\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 126us/step - loss: 0.6239 - acc: 0.6832\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 94us/step - loss: 0.6333 - acc: 0.6832\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 78us/step - loss: 0.6220 - acc: 0.6832\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 85us/step - loss: 0.6354 - acc: 0.6832\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 77us/step - loss: 0.6192 - acc: 0.6832\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 100us/step - loss: 0.6154 - acc: 0.6832\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 84us/step - loss: 0.6275 - acc: 0.6832\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 98us/step - loss: 0.6189 - acc: 0.6832\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 105us/step - loss: 0.6166 - acc: 0.6832\n",
            "101/101 [==============================] - 1s 13ms/step\n",
            "202/202 [==============================] - 0s 68us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 16ms/step - loss: 0.6932 - acc: 0.5099\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 117us/step - loss: 0.6935 - acc: 0.4802\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 98us/step - loss: 0.6939 - acc: 0.4851\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 94us/step - loss: 0.6926 - acc: 0.4901\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 97us/step - loss: 0.6941 - acc: 0.5099\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 94us/step - loss: 0.6948 - acc: 0.4703\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 93us/step - loss: 0.6919 - acc: 0.5248\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 92us/step - loss: 0.6931 - acc: 0.5297\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 102us/step - loss: 0.6945 - acc: 0.4604\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 103us/step - loss: 0.6933 - acc: 0.5099\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 111us/step - loss: 0.6939 - acc: 0.4703\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 89us/step - loss: 0.6919 - acc: 0.5050\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 98us/step - loss: 0.6957 - acc: 0.5000\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 108us/step - loss: 0.6953 - acc: 0.4950\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 98us/step - loss: 0.6918 - acc: 0.5000\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 96us/step - loss: 0.6942 - acc: 0.5099\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.6917 - acc: 0.5297\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 83us/step - loss: 0.6961 - acc: 0.4851\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 86us/step - loss: 0.6953 - acc: 0.4703\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 92us/step - loss: 0.6920 - acc: 0.5297\n",
            "101/101 [==============================] - 1s 13ms/step\n",
            "202/202 [==============================] - 0s 71us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 17ms/step - loss: 0.6838 - acc: 0.6980\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.6582 - acc: 0.8168\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 77us/step - loss: 0.6228 - acc: 0.8168\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 77us/step - loss: 0.5810 - acc: 0.8168\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 75us/step - loss: 0.5537 - acc: 0.8168\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 118us/step - loss: 0.4947 - acc: 0.8168\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.4968 - acc: 0.8168\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 79us/step - loss: 0.4953 - acc: 0.8168\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.5116 - acc: 0.8168\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 86us/step - loss: 0.4940 - acc: 0.8168\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 84us/step - loss: 0.5188 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 94us/step - loss: 0.5089 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.4864 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.4773 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 91us/step - loss: 0.5147 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 89us/step - loss: 0.4928 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 82us/step - loss: 0.4784 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 80us/step - loss: 0.5100 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 86us/step - loss: 0.4675 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.5005 - acc: 0.8168\n",
            "101/101 [==============================] - 1s 14ms/step\n",
            "202/202 [==============================] - 0s 58us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 3s 17ms/step - loss: 0.6940 - acc: 0.5149\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6847 - acc: 0.6782\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6772 - acc: 0.6832\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.6711 - acc: 0.6832\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 77us/step - loss: 0.6657 - acc: 0.6832\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 73us/step - loss: 0.6609 - acc: 0.6832\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6544 - acc: 0.6832\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6508 - acc: 0.6832\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 72us/step - loss: 0.6476 - acc: 0.6832\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6444 - acc: 0.6832\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.6417 - acc: 0.6832\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 73us/step - loss: 0.6395 - acc: 0.6832\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.6353 - acc: 0.6832\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6356 - acc: 0.6832\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6365 - acc: 0.6832\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6303 - acc: 0.6832\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6356 - acc: 0.6832\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 73us/step - loss: 0.6303 - acc: 0.6832\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6332 - acc: 0.6832\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6297 - acc: 0.6832\n",
            "101/101 [==============================] - 1s 14ms/step\n",
            "202/202 [==============================] - 0s 57us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 17ms/step - loss: 0.6937 - acc: 0.4851\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 82us/step - loss: 0.6935 - acc: 0.5050\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.6945 - acc: 0.4851\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 76us/step - loss: 0.6935 - acc: 0.4950\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.6943 - acc: 0.4901\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6930 - acc: 0.5099\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6922 - acc: 0.5149\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6930 - acc: 0.4851\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6931 - acc: 0.5248\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 77us/step - loss: 0.6915 - acc: 0.5050\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6899 - acc: 0.5149\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.6916 - acc: 0.5198\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.6928 - acc: 0.5099\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6948 - acc: 0.5000\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 82us/step - loss: 0.6929 - acc: 0.4901\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6914 - acc: 0.5446\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.6938 - acc: 0.5050\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6909 - acc: 0.5099\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6946 - acc: 0.5149\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6925 - acc: 0.5297\n",
            "101/101 [==============================] - 1s 15ms/step\n",
            "202/202 [==============================] - 0s 58us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 18ms/step - loss: 0.6747 - acc: 0.8020\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6321 - acc: 0.8168\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 77us/step - loss: 0.5600 - acc: 0.8168\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.5283 - acc: 0.8168\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 74us/step - loss: 0.5110 - acc: 0.8168\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 80us/step - loss: 0.5210 - acc: 0.8168\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 78us/step - loss: 0.5100 - acc: 0.8168\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.4959 - acc: 0.8168\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4819 - acc: 0.8168\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 79us/step - loss: 0.5045 - acc: 0.8168\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 75us/step - loss: 0.5098 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.4917 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 83us/step - loss: 0.5057 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.4950 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.4873 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.5201 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 85us/step - loss: 0.4870 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.4739 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 75us/step - loss: 0.4885 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.4912 - acc: 0.8168\n",
            "101/101 [==============================] - 2s 15ms/step\n",
            "202/202 [==============================] - 0s 69us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 18ms/step - loss: 0.6922 - acc: 0.5099\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 72us/step - loss: 0.6869 - acc: 0.6832\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6817 - acc: 0.6832\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.6761 - acc: 0.6832\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6700 - acc: 0.6832\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.6666 - acc: 0.6832\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6622 - acc: 0.6832\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6564 - acc: 0.6832\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6522 - acc: 0.6832\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.6491 - acc: 0.6832\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.6491 - acc: 0.6832\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6474 - acc: 0.6832\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6437 - acc: 0.6832\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6389 - acc: 0.6832\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6387 - acc: 0.6832\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 73us/step - loss: 0.6384 - acc: 0.6832\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.6337 - acc: 0.6832\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6340 - acc: 0.6832\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6319 - acc: 0.6832\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 80us/step - loss: 0.6256 - acc: 0.6832\n",
            "101/101 [==============================] - 2s 15ms/step\n",
            "202/202 [==============================] - 0s 67us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 19ms/step - loss: 0.6953 - acc: 0.5050\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6937 - acc: 0.4851\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6896 - acc: 0.5545\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 45us/step - loss: 0.6919 - acc: 0.5297\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.6914 - acc: 0.5545\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 44us/step - loss: 0.6887 - acc: 0.5149\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 83us/step - loss: 0.6961 - acc: 0.4851\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 72us/step - loss: 0.6916 - acc: 0.4851\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.6926 - acc: 0.5396\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.6938 - acc: 0.4653\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 80us/step - loss: 0.6942 - acc: 0.4703\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.6961 - acc: 0.4950\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.6916 - acc: 0.5099\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.6917 - acc: 0.5149\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.6907 - acc: 0.5297\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 50us/step - loss: 0.6965 - acc: 0.4703\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6937 - acc: 0.4950\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.6933 - acc: 0.4950\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.6921 - acc: 0.5198\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 53us/step - loss: 0.6946 - acc: 0.4802\n",
            "101/101 [==============================] - 2s 16ms/step\n",
            "202/202 [==============================] - 0s 39us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 19ms/step - loss: 0.6921 - acc: 0.5248\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 87us/step - loss: 0.6709 - acc: 0.8168\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.6518 - acc: 0.8168\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6342 - acc: 0.8168\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.6170 - acc: 0.8168\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6031 - acc: 0.8168\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.5811 - acc: 0.8168\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5657 - acc: 0.8168\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 68us/step - loss: 0.5505 - acc: 0.8168\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.5147 - acc: 0.8168\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4974 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 69us/step - loss: 0.5289 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.4984 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.5098 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.5190 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.5006 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.5088 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 75us/step - loss: 0.5148 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.4951 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.4808 - acc: 0.8168\n",
            "101/101 [==============================] - 2s 16ms/step\n",
            "202/202 [==============================] - 0s 47us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 20ms/step - loss: 0.6917 - acc: 0.6485\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.6824 - acc: 0.6832\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6742 - acc: 0.6832\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6718 - acc: 0.6832\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.6621 - acc: 0.6832\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.6619 - acc: 0.6832\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.6532 - acc: 0.6832\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6471 - acc: 0.6832\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6532 - acc: 0.6832\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.6516 - acc: 0.6832\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6482 - acc: 0.6832\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6423 - acc: 0.6832\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6419 - acc: 0.6832\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.6388 - acc: 0.6832\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.6366 - acc: 0.6832\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 79us/step - loss: 0.6364 - acc: 0.6832\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 65us/step - loss: 0.6372 - acc: 0.6832\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 76us/step - loss: 0.6298 - acc: 0.6832\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6309 - acc: 0.6832\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6298 - acc: 0.6832\n",
            "101/101 [==============================] - 2s 17ms/step\n",
            "202/202 [==============================] - 0s 44us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 20ms/step - loss: 0.6931 - acc: 0.5891\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6933 - acc: 0.5050\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.6936 - acc: 0.5000\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.6934 - acc: 0.5000\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6932 - acc: 0.5198\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 51us/step - loss: 0.6932 - acc: 0.5000\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6932 - acc: 0.5000\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6932 - acc: 0.5000\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 62us/step - loss: 0.6934 - acc: 0.5000\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.6932 - acc: 0.5000\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6932 - acc: 0.5000\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 78us/step - loss: 0.6932 - acc: 0.5000\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 75us/step - loss: 0.6933 - acc: 0.5000\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 67us/step - loss: 0.6933 - acc: 0.5000\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6933 - acc: 0.5000\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.6933 - acc: 0.5000\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 71us/step - loss: 0.6935 - acc: 0.5000\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 89us/step - loss: 0.6938 - acc: 0.5000\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.6938 - acc: 0.5000\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.6942 - acc: 0.5000\n",
            "101/101 [==============================] - 2s 17ms/step\n",
            "202/202 [==============================] - 0s 42us/step\n",
            "Epoch 1/20\n",
            "202/202 [==============================] - 4s 21ms/step - loss: 0.6974 - acc: 0.4059\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 0s 97us/step - loss: 0.6815 - acc: 0.8069\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 0s 61us/step - loss: 0.6569 - acc: 0.8168\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 0s 55us/step - loss: 0.6451 - acc: 0.8168\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 0s 73us/step - loss: 0.6202 - acc: 0.8168\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 0s 76us/step - loss: 0.5997 - acc: 0.8168\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 0s 63us/step - loss: 0.5767 - acc: 0.8168\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 0s 54us/step - loss: 0.5596 - acc: 0.8168\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 0s 47us/step - loss: 0.5369 - acc: 0.8168\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 0s 59us/step - loss: 0.5389 - acc: 0.8168\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 0s 60us/step - loss: 0.5311 - acc: 0.8168\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5220 - acc: 0.8168\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 0s 57us/step - loss: 0.5062 - acc: 0.8168\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 0s 56us/step - loss: 0.5138 - acc: 0.8168\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 0s 70us/step - loss: 0.5093 - acc: 0.8168\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 0s 58us/step - loss: 0.4979 - acc: 0.8168\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4926 - acc: 0.8168\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 0s 66us/step - loss: 0.4846 - acc: 0.8168\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 0s 81us/step - loss: 0.4858 - acc: 0.8168\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 0s 64us/step - loss: 0.4784 - acc: 0.8168\n",
            "101/101 [==============================] - 2s 17ms/step\n",
            "202/202 [==============================] - 0s 55us/step\n",
            "Epoch 1/20\n",
            "303/303 [==============================] - 4s 14ms/step - loss: 0.6935 - acc: 0.4554\n",
            "Epoch 2/20\n",
            "303/303 [==============================] - 0s 65us/step - loss: 0.6926 - acc: 0.5446\n",
            "Epoch 3/20\n",
            "303/303 [==============================] - 0s 68us/step - loss: 0.6914 - acc: 0.5578\n",
            "Epoch 4/20\n",
            "303/303 [==============================] - 0s 63us/step - loss: 0.6906 - acc: 0.5446\n",
            "Epoch 5/20\n",
            "303/303 [==============================] - 0s 68us/step - loss: 0.6909 - acc: 0.5446\n",
            "Epoch 6/20\n",
            "303/303 [==============================] - 0s 62us/step - loss: 0.6916 - acc: 0.5380\n",
            "Epoch 7/20\n",
            "303/303 [==============================] - 0s 64us/step - loss: 0.6912 - acc: 0.5446\n",
            "Epoch 8/20\n",
            "303/303 [==============================] - 0s 58us/step - loss: 0.6901 - acc: 0.5446\n",
            "Epoch 9/20\n",
            "303/303 [==============================] - 0s 54us/step - loss: 0.6899 - acc: 0.5446\n",
            "Epoch 10/20\n",
            "303/303 [==============================] - 0s 75us/step - loss: 0.6893 - acc: 0.5446\n",
            "Epoch 11/20\n",
            "303/303 [==============================] - 0s 69us/step - loss: 0.6896 - acc: 0.5446\n",
            "Epoch 12/20\n",
            "303/303 [==============================] - 0s 69us/step - loss: 0.6901 - acc: 0.5446\n",
            "Epoch 13/20\n",
            "303/303 [==============================] - 0s 63us/step - loss: 0.6903 - acc: 0.5446\n",
            "Epoch 14/20\n",
            "303/303 [==============================] - 0s 56us/step - loss: 0.6898 - acc: 0.5446\n",
            "Epoch 15/20\n",
            "303/303 [==============================] - 0s 55us/step - loss: 0.6899 - acc: 0.5446\n",
            "Epoch 16/20\n",
            "303/303 [==============================] - 0s 60us/step - loss: 0.6901 - acc: 0.5446\n",
            "Epoch 17/20\n",
            "303/303 [==============================] - 0s 57us/step - loss: 0.6899 - acc: 0.5446\n",
            "Epoch 18/20\n",
            "303/303 [==============================] - 0s 64us/step - loss: 0.6895 - acc: 0.5446\n",
            "Epoch 19/20\n",
            "303/303 [==============================] - 0s 52us/step - loss: 0.6903 - acc: 0.5446\n",
            "Epoch 20/20\n",
            "303/303 [==============================] - 0s 59us/step - loss: 0.6896 - acc: 0.5446\n",
            "Best: 0.21452144939120454 using {'batch_size': 60, 'epochs': 20}\n",
            "Means: 0.12211221200798211, Stdev: 0.172692746353067 with: {'batch_size': 10, 'epochs': 20}\n",
            "Means: 0.1716171625030316, Stdev: 0.24270331874777468 with: {'batch_size': 20, 'epochs': 20}\n",
            "Means: 0.12211221200798211, Stdev: 0.172692746353067 with: {'batch_size': 40, 'epochs': 20}\n",
            "Means: 0.21452144939120454, Stdev: 0.30337914314897496 with: {'batch_size': 60, 'epochs': 20}\n",
            "Means: 0.2112211252596512, Stdev: 0.29871178000190507 with: {'batch_size': 80, 'epochs': 20}\n",
            "Means: 0.21122111739105123, Stdev: 0.2987117688740243 with: {'batch_size': 100, 'epochs': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fxztT1F8kYO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0iPnxHLkYRh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}